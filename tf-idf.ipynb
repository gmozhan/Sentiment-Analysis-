{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "definition of generative models :\n",
    "\n",
    "Generative classifiers learn a model of the joint probability, p(x,y), of the inputs x and the label y, and make their predictions by using Bayes rules to calculate p(y|x), and then picking the most likely label y.(a generative model is a model of the conditional probability of the observable X, given a target y, symbolically,P(X|Y=y))\n",
    "\n",
    "definition of discriminative models:\n",
    "\n",
    "A discriminative model is a model of the conditional probability of the target Y, given an observation x, symbolically, P(Y|X=x).\n",
    "\n",
    "https://en.wikipedia.org/wiki/Generative_model#CITEREFNgJordan2002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "difference:\n",
    "\n",
    "A Discriminative model,models the decision boundary between the classes. A Generative Model explicitly models the actual distribution of each class.\n",
    "\n",
    "In practice, the generalization performance of generative models is often found to be poorer than of discriminative models due to diﬀerences between the model and the true distribution of the data.\n",
    "\n",
    "When labelled training data is plentiful, discriminative techniques are widely used since they give excellent generalization performance.Discriminative methods give good predictive performance and have been widely used in many applications.\n",
    "\n",
    "A discriminative model cannot make use of the unlabelled data, and so in this case we need to consider a generative approach.\n",
    "\n",
    "the logistic regression (the discriminative counterpart of a Naive Bayes generative model) works better than its generative counterpart, but only for a large number of training data points (large depending on the complexity of the problem).\n",
    "\n",
    "The discriminative model will work very well as only one set of parameter has to be learned.The discriminative model will take less time as only one set of parameter has to be learned.The discriminative model will take less time as prediction can be performed directly by the learned conditional probability. The generative model additionaly needs to apply Bayes’ Theorem.The generative models fail to work well if we give a lot of features.The generative model prone to overfitting very easily.The discriminative model prone to underfitting easily.The discriminative model is more sensitive to outliers as we model the boundaries. Each outlier will shift the class boundaries.\n",
    "\n",
    "http://ai.stanford.edu/~ang/papers/nips01-discriminativegenerative.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1)True\n",
    "\n",
    "naive bayes is considered a generative model Because it computes p(x,y) rather than p(y|x).and logistic regression is a discriminative model because it computes p(y|x) directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2)True\n",
    "\n",
    "consider a GNB based on the following modeling assumptions:\n",
    "\n",
    "1-Y is boolean, governed by a Bernoulli distribution, with parameter π=P(Y=1)\n",
    "\n",
    "2-X=〈X1…Xn〉, where each Xi is a continuous random variable\n",
    "\n",
    "3-For each Xi, P(Xi|Y=yk) is a Gaussian distribution of the form N(μik,σi)\n",
    "\n",
    "4-For all i and j not eq i, Xi and Xj are conditionally independent given Y\n",
    "\n",
    "P(Y|X) can be expressed in the parametric form given by Logistic Regression, under the Gaussian Naive Bayes assumptions detailed here.\n",
    "\n",
    "https://appliedmachinelearning.blog/2019/09/30/equivalence-of-gaussian-naive-bayes-and-logistic-regression-an-explanation/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3)Flase \n",
    "\n",
    "Logistic regression is considered a generalized linear model because the outcome always depends on the sum of the inputs and parameters. Or in other words, the output cannot depend on the product (or quotient, etc.) of its parameters.\n",
    "\n",
    "in general the naive Bayes classifier is not linear, but if the likelihood factors p(xi∣c) are from exponential families, the naive Bayes classifier corresponds to a linear classifier in a particular feature space.\n",
    "\n",
    "https://stats.stackexchange.com/questions/142215/how-is-naive-bayes-a-linear-classifier#:~:targetText=In%20general%20the%20naive%20Bayes,in%20a%20particular%20feature%20space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4)True\n",
    "\n",
    "However one layer perceptron uses gradient descent to update weights but logestic regression uses gradient ascent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.5)False\n",
    "\n",
    "Our goal is to choosing parameters that maximize likelihood,and we can do this by gradient ascent.\n",
    "The idea behind gradient ascent is that gradients point “uphill”. If you continuously take small steps in the direction of your gradient, you will eventually make it to a local maximum. \n",
    "\n",
    "https://web.stanford.edu/class/archive/cs/cs109/cs109.1178/lectureHandouts/220-logistic-regression.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1)\n",
    "\n",
    "\\begin{array}{l}\n",
    "{V_{MAP}} = \\arg \\mathop {\\max }\\limits_{{v_j} \\in \\{ A,B\\}} P({v_j})\\prod\\limits_{i = 1}^n {P({a_i}|{v_j})} \n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d1 : {x=1, y=1, z=1, w=0}\n",
    "\n",
    "\n",
    "\\begin{array}{l}\n",
    "P(A)\\prod\\limits_{i = 1}^n {P({a_i}|{v_j})}  = P(A).P(x = 1|A).P(y = 1|A).P(z = 1|A).P(w = 0|A)\\\\\n",
    " = \\frac{5}{9} \\cdot \\frac{3}{5} \\cdot \\frac{4}{5} \\cdot \\frac{4}{5} \\cdot \\frac{2}{5} = 0.0853\\\\\n",
    "P(B)\\prod\\limits_{i = 1}^n {P({a_i}|{v_j})}  = P(B).P(x = 1|B).P(y = 1|B).P(z = 1|B).P(w = 0|B)\\\\\n",
    " = \\frac{4}{9} \\cdot \\frac{3}{4} \\cdot \\frac{0}{4} \\cdot \\frac{1}{4} \\cdot \\frac{1}{4} = 0\n",
    "\\end{array}\n",
    "\n",
    "so class(d1)=A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d2:\n",
    "\n",
    "\\begin{array}{l}\n",
    "P(A)\\prod\\limits_{i = 1}^n {P({a_i}|{v_j})}  = P(A).P(x = 1|A).P(y = 0|A).P(z = 0|A).P(w = 1|A)\\\\\n",
    " = \\frac{5}{9} \\cdot \\frac{3}{5} \\cdot \\frac{1}{5} \\cdot \\frac{1}{5} \\cdot \\frac{3}{5} = 0.008\\\\\n",
    "P(B)\\prod\\limits_{i = 1}^n {P({a_i}|{v_j})}  = P(B).P(x = 1|B).P(y = 0|B).P(z = 0|B).P(w = 1|B)\\\\\n",
    " = \\frac{4}{9} \\cdot \\frac{3}{4} \\cdot \\frac{4}{4} \\cdot \\frac{3}{4} \\cdot \\frac{3}{4} = {\\rm{0}}{\\rm{.1875}}\n",
    "\\end{array}\n",
    "\n",
    "class(d2)=B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d3:\n",
    "\n",
    "\\begin{array}{l}\n",
    "P(A)\\prod\\limits_{i = 1}^n {P({a_i}|{v_j})}  = P(A).P(x = 0|A).P(y = 0|A).P(z = 1|A).P(w = 1|A)\\\\\n",
    " = \\frac{5}{9} \\cdot \\frac{2}{5} \\cdot \\frac{1}{5} \\cdot \\frac{4}{5} \\cdot \\frac{3}{5} = {\\rm{0}}{\\rm{.0213}}\\\\\n",
    "P(B)\\prod\\limits_{i = 1}^n {P({a_i}|{v_j})}  = P(B).P(x = 0|B).P(y = 0|B).P(z = 1|B).P(w = 1|B)\\\\\n",
    " = \\frac{4}{9} \\cdot \\frac{1}{4} \\cdot \\frac{4}{4} \\cdot \\frac{1}{4} \\cdot \\frac{3}{4} = {\\rm{0}}{\\rm{.0208}}\n",
    "\\end{array}\n",
    "\n",
    "class(d3)=A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d4 :\n",
    "\n",
    "\n",
    "\\begin{array}{l}\n",
    "P(A)\\prod\\limits_{i = 1}^n {P({a_i}|{v_j})}  = P(A).P(x = 1|A).P(y = 1|A).P(z = 0|A).P(w = 1|A)\\\\\n",
    " = \\frac{5}{9} \\cdot \\frac{3}{5} \\cdot \\frac{4}{5} \\cdot \\frac{1}{5} \\cdot \\frac{3}{5} = {\\rm{0}}{\\rm{.032}}\\\\\n",
    "P(B)\\prod\\limits_{i = 1}^n {P({a_i}|{v_j})}  = P(B).P(x = 1|B).P(y = 1|B).P(z = 0|B).P(w = 1|B)\\\\\n",
    " = \\frac{4}{9} \\cdot \\frac{3}{4} \\cdot \\frac{0}{4} \\cdot \\frac{3}{4} \\cdot \\frac{3}{4} = {\\rm{0}}\n",
    "\\end{array}\n",
    "\n",
    "class(d4)=A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d5:\n",
    "\n",
    "\\begin{array}{l}\n",
    "P(A)\\prod\\limits_{i = 1}^n {P({a_i}|{v_j})}  = P(A).P(x = 1|A).P(y = 0|A).P(z = 0|A).P(w = 0|A)\\\\\n",
    " = \\frac{5}{9} \\cdot \\frac{3}{5} \\cdot \\frac{1}{5} \\cdot \\frac{1}{5} \\cdot \\frac{2}{5} = {\\rm{0}}{\\rm{.0053}}\\\\\n",
    "P(B)\\prod\\limits_{i = 1}^n {P({a_i}|{v_j})}  = P(B).P(x = 1|B).P(y = 0|B).P(z = 0|B).P(w = 0|B)\\\\\n",
    " = \\frac{4}{9} \\cdot \\frac{3}{4} \\cdot \\frac{4}{4} \\cdot \\frac{3}{4} \\cdot \\frac{1}{4} = {\\rm{0}}{\\rm{.0625}}\n",
    "\\end{array}\n",
    "\n",
    "class(d5)=B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d6:\n",
    "\n",
    "\\begin{array}{l}\n",
    "P(A)\\prod\\limits_{i = 1}^n {P({a_i}|{v_j})}  = P(A).P(x = 0|A).P(y = 1|A).P(z = 1|A).P(w = 0|A)\\\\\n",
    " = \\frac{5}{9} \\cdot \\frac{2}{5} \\cdot \\frac{4}{5} \\cdot \\frac{4}{5} \\cdot \\frac{2}{5} = {\\rm{0}}{\\rm{.0568}}\\\\\n",
    "P(B)\\prod\\limits_{i = 1}^n {P({a_i}|{v_j})}  = P(B).P(x = 0|B).P(y = 1|B).P(z = 1|B).P(w = 0|B)\\\\\n",
    " = \\frac{4}{9} \\cdot \\frac{1}{4} \\cdot \\frac{0}{4} \\cdot \\frac{1}{4} \\cdot \\frac{1}{4} = {\\rm{0}}\n",
    "\\end{array}\n",
    "\n",
    "class(d6)=A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d7:\n",
    "\n",
    "\\begin{array}{l}\n",
    "P(A)\\prod\\limits_{i = 1}^n {P({a_i}|{v_j})}  = P(A).P(x = 0|A).P(y = 0|A).P(z = 0|A).P(w = 1|A)\\\\\n",
    " = \\frac{5}{9} \\cdot \\frac{2}{5} \\cdot \\frac{1}{5} \\cdot \\frac{1}{5} \\cdot \\frac{3}{5} = {\\rm{0}}{\\rm{.0053}}\\\\\n",
    "P(B)\\prod\\limits_{i = 1}^n {P({a_i}|{v_j})}  = P(B).P(x = 0|B).P(y = 0|B).P(z = 0|B).P(w = 1|B)\\\\\n",
    " = \\frac{4}{9} \\cdot \\frac{1}{4} \\cdot \\frac{4}{4} \\cdot \\frac{3}{4} \\cdot \\frac{3}{4} = {\\rm{0}}{\\rm{.0625}}\n",
    "\\end{array}\n",
    "\n",
    "class(d7)=B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d8:\n",
    "\n",
    "\\begin{array}{l}\n",
    "P(A)\\prod\\limits_{i = 1}^n {P({a_i}|{v_j})}  = P(A).P(x = 1|A).P(y = 0|A).P(z = 1|A).P(w = 1|A)\\\\\n",
    " = \\frac{5}{9} \\cdot \\frac{3}{5} \\cdot \\frac{1}{5} \\cdot \\frac{4}{5} \\cdot \\frac{3}{5} = {\\rm{0}}{\\rm{.032}}\\\\\n",
    "P(B)\\prod\\limits_{i = 1}^n {P({a_i}|{v_j})}  = P(B).P(x = 1|B).P(y = 0|B).P(z = 1|B).P(w = 1|B)\\\\\n",
    " = \\frac{4}{9} \\cdot \\frac{3}{4} \\cdot \\frac{4}{4} \\cdot \\frac{1}{4} \\cdot \\frac{3}{4} = {\\rm{0}}{\\rm{.0625}}\n",
    "\\end{array}\n",
    "\n",
    "class(d8)=B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d9:\n",
    "\n",
    "\\begin{array}{l}\n",
    "P(A)\\prod\\limits_{i = 1}^n {P({a_i}|{v_j})}  = P(A).P(x = 1|A).P(y = 1|A).P(z = 1|A).P(w = 1|A)\\\\\n",
    " = \\frac{5}{9} \\cdot \\frac{3}{5} \\cdot \\frac{4}{5} \\cdot \\frac{4}{5} \\cdot \\frac{3}{5} = {\\rm{0}}{\\rm{.128}}\\\\\n",
    "P(B)\\prod\\limits_{i = 1}^n {P({a_i}|{v_j})}  = P(B).P(x = 1|B).P(y = 1|B).P(z = 1|B).P(w = 1|B)\\\\\n",
    " = \\frac{4}{9} \\cdot \\frac{3}{4} \\cdot \\frac{0}{4} \\cdot \\frac{1}{4} \\cdot \\frac{3}{4} = {\\rm{0}}\n",
    "\\end{array}\n",
    "\n",
    "class(d9)=A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on train examples : accuracy =100%  error=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2)\n",
    "\n",
    "d10:\n",
    "\n",
    "\\begin{array}{l}\n",
    "P(A)\\prod\\limits_{i = 1}^n {P({a_i}|{v_j})}  = P(A).P(x = 1|A).P(y = 1|A).P(z = 0|A).P(w = 0|A)\\\\\n",
    " = \\frac{5}{9} \\cdot \\frac{3}{5} \\cdot \\frac{4}{5} \\cdot \\frac{1}{5} \\cdot \\frac{2}{5} = {\\rm{0}}{\\rm{.0213}}\\\\\n",
    "P(B)\\prod\\limits_{i = 1}^n {P({a_i}|{v_j})}  = P(B).P(x = 1|B).P(y = 1|B).P(z = 0|B).P(w = 0|B)\\\\\n",
    " = \\frac{4}{9} \\cdot \\frac{3}{4} \\cdot \\frac{0}{4} \\cdot \\frac{3}{4} \\cdot \\frac{1}{4} = {\\rm{0}}\n",
    "\\end{array}\n",
    "\n",
    "class(d10)=A (the real class is B) (FP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d11:\n",
    "\n",
    "\\begin{array}{l}\n",
    "P(A)\\prod\\limits_{i = 1}^n {P({a_i}|{v_j})}  = P(A).P(x = 0|A).P(y = 0|A).P(z = 0|A).P(w = 0|A)\\\\\n",
    " = \\frac{5}{9} \\cdot \\frac{2}{5} \\cdot \\frac{1}{5} \\cdot \\frac{1}{5} \\cdot \\frac{2}{5} = {\\rm{0}}{\\rm{.0035}}\\\\\n",
    "P(B)\\prod\\limits_{i = 1}^n {P({a_i}|{v_j})}  = P(B).P(x = 0|B).P(y = 0|B).P(z = 0|B).P(w = 0|B)\\\\\n",
    " = \\frac{4}{9} \\cdot \\frac{1}{4} \\cdot \\frac{4}{4} \\cdot \\frac{3}{4} \\cdot \\frac{1}{4} = {\\rm{0}}{\\rm{.0208}}\n",
    "\\end{array}\n",
    "\n",
    "class(d11)=B (TN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d12:\n",
    "\n",
    "\\begin{array}{l}\n",
    "P(A)\\prod\\limits_{i = 1}^n {P({a_i}|{v_j})}  = P(A).P(x = 0|A).P(y = 0|A).P(z = 1|A).P(w = 0|A)\\\\\n",
    " = \\frac{5}{9} \\cdot \\frac{2}{5} \\cdot \\frac{1}{5} \\cdot \\frac{4}{5} \\cdot \\frac{2}{5} = {\\rm{0}}{\\rm{.0142}}\\\\\n",
    "P(B)\\prod\\limits_{i = 1}^n {P({a_i}|{v_j})}  = P(B).P(x = 0|B).P(y = 0|B).P(z = 1|B).P(w = 0|B)\\\\\n",
    " = \\frac{4}{9} \\cdot \\frac{1}{4} \\cdot \\frac{4}{4} \\cdot \\frac{1}{4} \\cdot \\frac{1}{4} = {\\rm{0}}{\\rm{.0069}}\n",
    "\\end{array}\n",
    "\n",
    "class(d12)=A (TP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3) confusion matrix:\n",
    "\n",
    "\\begin{array}{l}\n",
    "\\left( {\\begin{array}{*{20}{c}}\n",
    "{TP = 1}&{FP = 1}\\\\\n",
    "{TN = 1}&{FN = 0}\n",
    "\\end{array}} \\right)\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.4)\n",
    "\n",
    "Precision = TP/(TP+FP) = 1/2\n",
    "\n",
    "Recall = TP/(TP+FN) = 1\n",
    "\n",
    "F1 Score = 2*(Recall * Precision)/(Recall + Precision) = 2(0.5/1.5)= 0.66\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# implementing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opening data file('imdb_labelled.txt')\n",
    "with open('imdb_labelled.txt') as f:\n",
    "    data=f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews=[]\n",
    "labels=[]\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spliting reviews from labels\n",
    "for review in data:\n",
    "    temp=review.split('\\t')\n",
    "    reviews.append(temp[0])\n",
    "    labels.append(int(temp[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentense embedding using tf-idf\n",
    "# pre-processing using tokenizer from nltk\n",
    "tfidf = TfidfVectorizer(tokenizer=tokenizer.tokenize,stop_words=stop_words)  \n",
    "tfidf.fit(reviews)\n",
    "vectors=[]\n",
    "for r in reviews:\n",
    "    # turn every review to a vector\n",
    "    vector=tfidf.transform([r]).toarray()[0]\n",
    "    vectors.append(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(vectors, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## implementing Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.752\n",
      "Error : 0.248\n",
      "F1_Score : 0.7615384615384616\n",
      "Confusion matrix : \n",
      "[[89 42]\n",
      " [20 99]]\n"
     ]
    }
   ],
   "source": [
    "clf_nb = MultinomialNB().fit(X_train,Y_train)\n",
    "Y_pred=clf_nb.predict(X_test)\n",
    "accu=accuracy_score(Y_test,Y_pred)\n",
    "print('Accuracy : ' + str(accu) )\n",
    "print('Error : '+ str(1-accu))\n",
    "print('F1_Score : '+str(f1_score(Y_test,Y_pred)))\n",
    "print('Confusion matrix : \\n'+str(confusion_matrix(Y_test,Y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## implementing Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.732\n",
      "Error : 0.268\n",
      "F1_Score : 0.7490636704119851\n",
      "Confusion matrix : \n",
      "[[ 83  48]\n",
      " [ 19 100]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "clf_lr = LogisticRegression().fit(X_train, Y_train)\n",
    "Y_pred=clf_lr.predict(X_test)\n",
    "accu=accuracy_score(Y_test,Y_pred)\n",
    "print('Accuracy : ' + str(accu) )\n",
    "print('Error : '+ str(1-accu))\n",
    "print('F1_Score : '+str(f1_score(Y_test,Y_pred)))\n",
    "print('Confusion matrix : \\n'+str(confusion_matrix(Y_test,Y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### in this case Naive Bayes works better than Logistic Regression and it's because Naive Bayes works better with small training data sets, but  when the training size reaches infinity the Logistic Regression performs better than the Naive Bayes. (it depends on data set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
